{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangho-kim/LLM-AD/blob/master/%EC%98%A4%ED%86%A0%EC%9D%B8%EC%BD%94%EB%8D%94%20%EB%B0%8F%20SHAP%20%EA%B8%B0%EB%B0%98%20FDC_%EB%B6%84%EC%84%9D_%EB%AA%A8%EB%8D%B8_(%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%94%84%EB%A0%88%EC%9E%84_%EB%B2%84%EC%A0%84).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 패키지 설치 (Package Installation)\n",
        "# ==============================================================================\n",
        "# 아래 명령어를 사용하여 이 스크립트 실행에 필요한 주요 라이브러리들을 설치합니다.\n",
        "# In a command line or terminal, run the following command:\n",
        "# pip install pandas numpy tensorflow scikit-learn shap matplotlib seaborn koreanize-matplotlib\n",
        "\n",
        "# ==============================================================================\n",
        "# 라이브러리 임포트 (Library Imports)\n",
        "# ==============================================================================\n",
        "# 데이터 처리, 모델링, 시각화, 설명가능 AI에 필요한 라이브러리들을 임포트합니다.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "# Matplotlib에서 한글 폰트가 깨지지 않도록 설정\n",
        "import koreanize_matplotlib\n",
        "\n",
        "# 경고 메시지 무시\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==============================================================================\n",
        "# 1단계: 샘플 데이터 생성 (Sample Data Generation)\n",
        "# ==============================================================================\n",
        "# FDC 데이터를 딕셔너리가 아닌 데이터프레임 형식으로 생성하도록 수정합니다.\n",
        "def generate_wafer_data_df(num_wafers=500, timesteps=100):\n",
        "    \"\"\"\n",
        "    하나의 공정 스텝에 대한 웨이퍼 데이터를 생성합니다.\n",
        "    - FDC: 4개 센서 데이터를 포함하는 단일 데이터프레임 (Long Format)\n",
        "    - MES: 각 웨이퍼의 공정 조건 및 최종 수율 정보\n",
        "    \"\"\"\n",
        "    print(f\"{num_wafers}개 웨이퍼에 대한 샘플 데이터 생성을 시작합니다...\")\n",
        "\n",
        "    # --- MES 데이터 생성 (정적 특징) ---\n",
        "    wafers = []\n",
        "    for i in range(num_wafers):\n",
        "        wafer_id = f'WAF_{i+1:04d}'\n",
        "        equipment_id = np.random.choice(['Etcher_A', 'Etcher_B', 'Etcher_C'])\n",
        "        recipe_id = np.random.choice(['Recipe_1', 'Recipe_2'])\n",
        "        wafers.append([wafer_id, equipment_id, recipe_id])\n",
        "    mes_df = pd.DataFrame(wafers, columns=['wafer_id', 'equipment_id', 'recipe_id'])\n",
        "\n",
        "    # --- FDC 데이터프레임 생성 ---\n",
        "    fdc_records = []\n",
        "    anomaly_types = []\n",
        "    final_yields = []\n",
        "\n",
        "    for i, row in mes_df.iterrows():\n",
        "        wafer_id = row['wafer_id']\n",
        "\n",
        "        # 기본 센서 값 설정\n",
        "        pressure_mean, rf_power_mean, gas_flow_mean, temp_mean = 100, 500, 200, 80\n",
        "        pressure = np.random.normal(pressure_mean, 1, timesteps)\n",
        "        rf_power = np.random.normal(rf_power_mean, 5, timesteps)\n",
        "        gas_flow = np.random.normal(gas_flow_mean, 2, timesteps)\n",
        "        temperature = np.random.normal(temp_mean, 0.5, timesteps)\n",
        "        anomaly_type, final_yield = 'Normal', 1\n",
        "\n",
        "        if np.random.rand() < 0.2:\n",
        "            final_yield = 0\n",
        "            anomaly_choice = np.random.choice(['Spike', 'Variance', 'Drift'])\n",
        "            anomaly_type = anomaly_choice\n",
        "            if anomaly_choice == 'Spike':\n",
        "                spike_sensor = np.random.choice(['pressure', 'rf_power'])\n",
        "                spike_idx = np.random.randint(20, 80)\n",
        "                if spike_sensor == 'pressure': pressure[spike_idx] += pressure_mean * 0.1 * np.sign(np.random.randn())\n",
        "                else: rf_power[spike_idx] += rf_power_mean * 0.05 * np.sign(np.random.randn())\n",
        "            elif anomaly_choice == 'Variance':\n",
        "                var_sensor = np.random.choice(['gas_flow', 'temperature'])\n",
        "                if var_sensor == 'gas_flow': gas_flow = np.random.normal(gas_flow_mean, 6, timesteps)\n",
        "                else: temperature = np.random.normal(temp_mean, 1.5, timesteps)\n",
        "            elif anomaly_choice == 'Drift':\n",
        "                drift_sensor = np.random.choice(['pressure', 'rf_power'])\n",
        "                drift_amount = np.linspace(0, (np.random.rand() - 0.5) * 10, timesteps)\n",
        "                if drift_sensor == 'pressure': pressure += drift_amount\n",
        "                else: rf_power += drift_amount * 5\n",
        "\n",
        "        # 해당 웨이퍼의 FDC 데이터를 레코드 리스트에 추가\n",
        "        for t in range(timesteps):\n",
        "            fdc_records.append({\n",
        "                'wafer_id': wafer_id,\n",
        "                'timestep': t,\n",
        "                'pressure': pressure[t],\n",
        "                'rf_power': rf_power[t],\n",
        "                'gas_flow': gas_flow[t],\n",
        "                'temperature': temperature[t]\n",
        "            })\n",
        "\n",
        "        anomaly_types.append(anomaly_type)\n",
        "        final_yields.append(final_yield)\n",
        "\n",
        "    # 최종 데이터프레임 생성\n",
        "    fdc_df = pd.DataFrame(fdc_records)\n",
        "    mes_df['anomaly_type'] = anomaly_types\n",
        "    mes_df['final_yield'] = final_yields\n",
        "\n",
        "    print(\"샘플 데이터 생성이 완료되었습니다.\")\n",
        "    return mes_df, fdc_df\n",
        "\n",
        "# 데이터 생성 실행\n",
        "mes_df, fdc_df = generate_wafer_data_df(num_wafers=500, timesteps=100)\n",
        "print(\"\\n[생성된 FDC 데이터프레임 샘플 (상위 5개)]\")\n",
        "print(fdc_df.head())\n",
        "\n",
        "# ==============================================================================\n",
        "# 2단계: 시계열 특징 공학 (피처 강화)\n",
        "# ==============================================================================\n",
        "# 데이터프레임을 입력으로 받도록 특징 공학 함수를 수정합니다.\n",
        "def feature_engineering_enhanced_df(mes_df, fdc_df, timesteps=100):\n",
        "    \"\"\" 데이터프레임 형식의 시계열 데이터로부터 특징을 추출합니다. \"\"\"\n",
        "    print(\"\\n강화된 시계열 특징 공학을 시작합니다 (데이터프레임 입력)...\")\n",
        "\n",
        "    features_list = []\n",
        "    stable_start, stable_end = int(timesteps * 0.2), int(timesteps * 0.8)\n",
        "\n",
        "    # wafer_id로 그룹화하여 웨이퍼별로 특징 추출\n",
        "    grouped = fdc_df.groupby('wafer_id')\n",
        "\n",
        "    for wafer_id, group_df in grouped:\n",
        "        wafer_features = {'wafer_id': wafer_id}\n",
        "        for sensor in ['pressure', 'rf_power', 'gas_flow', 'temperature']:\n",
        "            series = group_df[sensor].values\n",
        "\n",
        "            # 기본 통계 피처\n",
        "            wafer_features[f'{sensor}_mean'] = np.mean(series)\n",
        "            wafer_features[f'{sensor}_std'] = np.std(series)\n",
        "            wafer_features[f'{sensor}_max'] = np.max(series)\n",
        "            wafer_features[f'{sensor}_min'] = np.min(series)\n",
        "            wafer_features[f'{sensor}_skew'] = pd.Series(series).skew()\n",
        "            wafer_features[f'{sensor}_kurt'] = pd.Series(series).kurtosis()\n",
        "\n",
        "            # 도메인 특화 피처\n",
        "            wafer_features[f'{sensor}_stable_std'] = np.std(series[stable_start:stable_end])\n",
        "            wafer_features[f'{sensor}_auc'] = np.trapz(series)\n",
        "            wafer_features[f'{sensor}_slope'] = np.polyfit(range(timesteps), series, 1)[0]\n",
        "            peak_threshold = np.mean(series) + 3 * np.std(series)\n",
        "            wafer_features[f'{sensor}_peak_count'] = np.sum(series > peak_threshold)\n",
        "\n",
        "        features_list.append(wafer_features)\n",
        "\n",
        "    features_df = pd.DataFrame(features_list)\n",
        "    print(\"특징 추출이 완료되었습니다.\")\n",
        "\n",
        "    final_df = pd.merge(mes_df, features_df, on='wafer_id')\n",
        "    print(\"동적/정적 데이터 통합이 완료되었습니다.\")\n",
        "    return final_df\n",
        "\n",
        "# 강화된 특징 공학 실행\n",
        "final_df = feature_engineering_enhanced_df(mes_df, fdc_df)\n",
        "print(\"\\n[최종 통합 데이터셋 샘플 (상위 5개)]\")\n",
        "print(final_df.head())\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3단계: 비선형 이상 탐지 (오토인코더 모델)\n",
        "# ==============================================================================\n",
        "# 이 단계는 2단계의 결과물인 final_df를 사용하므로 이전과 동일하게 작동합니다.\n",
        "\n",
        "# --- 데이터 전처리 ---\n",
        "features_for_model = [col for col in final_df.columns if col not in ['wafer_id', 'anomaly_type', 'final_yield']]\n",
        "categorical_features = ['equipment_id', 'recipe_id']\n",
        "numerical_features = [col for col in features_for_model if col not in categorical_features]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ], remainder='passthrough')\n",
        "\n",
        "normal_df = final_df[final_df['final_yield'] == 1]\n",
        "X_train, X_val = train_test_split(normal_df[features_for_model], test_size=0.2, random_state=42)\n",
        "preprocessor.fit(final_df[features_for_model])\n",
        "X_train_scaled = preprocessor.transform(X_train)\n",
        "X_val_scaled = preprocessor.transform(X_val)\n",
        "X_all_scaled = preprocessor.transform(final_df[features_for_model])\n",
        "\n",
        "# --- 오토인코더 모델 정의 및 학습 ---\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "encoding_dim = int(input_dim / 2)\n",
        "input_layer = Input(shape=(input_dim,)); encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
        "encoder = Dense(int(encoding_dim/2), activation=\"relu\")(encoder)\n",
        "decoder = Dense(int(encoding_dim/2), activation='relu')(encoder); decoder = Dense(encoding_dim, activation='relu')(decoder)\n",
        "output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "autoencoder.compile(optimizer='adam', loss='mae')\n",
        "print(\"\\n오토인코더 모델 학습을 시작합니다...\")\n",
        "history = autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=16, shuffle=True, validation_data=(X_val_scaled, X_val_scaled),\n",
        "                          callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")], verbose=1).history\n",
        "\n",
        "# --- 이상 점수 계산 및 평가 ---\n",
        "X_pred_scaled = autoencoder.predict(X_all_scaled)\n",
        "mae = np.mean(np.abs(X_all_scaled - X_pred_scaled), axis=1)\n",
        "final_df['reconstruction_error'] = mae\n",
        "train_mae = np.mean(np.abs(X_train_scaled - autoencoder.predict(X_train_scaled)), axis=1)\n",
        "threshold = np.quantile(train_mae, 0.99)\n",
        "print(f\"\\n계산된 이상 탐지 임계치: {threshold:.4f}\")\n",
        "\n",
        "y_true = final_df['final_yield'] == 0\n",
        "y_pred = final_df['reconstruction_error'] > threshold\n",
        "print(\"\\n[이상 탐지 모델 평가 결과 (오토인코더)]\")\n",
        "print(classification_report(y_true, y_pred, target_names=['정상 (Normal)', '이상 (Anomaly)']))\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['정상 (Normal)', '이상 (Anomaly)'], yticklabels=['정상 (Normal)', '이상 (Anomaly)'])\n",
        "plt.title('혼동 행렬 (Confusion Matrix) - 오토인코더', fontsize=16); plt.ylabel('실제 값 (Actual)', fontsize=12); plt.xlabel('예측 값 (Predicted)', fontsize=12); plt.show()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4단계: 원인 변수 설명 (Explainable AI with SHAP)\n",
        "# ==============================================================================\n",
        "# 이 단계 역시 final_df를 사용하므로 이전과 동일하게 작동합니다.\n",
        "print(\"\\nSHAP 분석을 시작합니다 (상위 5개 이상 샘플 대상)...\")\n",
        "ohe_feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
        "all_feature_names = numerical_features + ohe_feature_names\n",
        "def model_predict_loss(data): return np.mean(np.abs(data - autoencoder.predict(data)), axis=1)\n",
        "background_data = shap.sample(X_train_scaled, 100)\n",
        "explainer = shap.KernelExplainer(model_predict_loss, background_data)\n",
        "anomaly_samples_df = final_df[y_pred].head(5)\n",
        "if not anomaly_samples_df.empty:\n",
        "    anomaly_samples_scaled = preprocessor.transform(anomaly_samples_df[features_for_model])\n",
        "    shap_values = explainer.shap_values(anomaly_samples_scaled)\n",
        "    X_anomaly_df = pd.DataFrame(anomaly_samples_scaled, columns=all_feature_names)\n",
        "    print(\"\\n각 이상 샘플에 대한 SHAP Force Plot:\")\n",
        "    for i in range(len(shap_values)):\n",
        "        shap.force_plot(explainer.expected_value, shap_values[i, :], X_anomaly_df.iloc[i, :], matplotlib=True, show=True)\n",
        "    print(\"\\n전체 이상 샘플에 대한 SHAP Summary Plot:\")\n",
        "    shap.summary_plot(shap_values, X_anomaly_df)\n",
        "else:\n",
        "    print(\"\\n탐지된 이상 샘플이 없어 SHAP 분석을 건너뜁니다.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "AFumKwWo4d7z"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}