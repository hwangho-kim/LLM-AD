{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangho-kim/LLM-AD/blob/master/%ED%95%98%EC%9D%B4%EB%B8%8C%EB%A6%AC%EB%93%9C_FDC_%EB%B6%84%EC%84%9D_%EB%AA%A8%EB%8D%B8_Python_%EA%B5%AC%ED%98%84%EC%B2%B4_(%EC%98%81%EB%AC%B8_%EA%B7%B8%EB%9E%98%ED%94%84).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 라이브러리 임포트 (Library Imports)\n",
        "# ==============================================================================\n",
        "# 데이터 처리, 모델링, 시각화, 설명가능 AI에 필요한 라이브러리들을 임포트합니다.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# 경고 메시지 무시\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==============================================================================\n",
        "# 1단계: 샘플 데이터 생성 (Sample Data Generation)\n",
        "# ==============================================================================\n",
        "# 실제 반도체 공정 데이터를 모사하여 FDC 시계열 데이터와 MES 정적 데이터를 생성합니다.\n",
        "# 정상 데이터와 함께 다양한 유형의 이상 데이터를 포함하여 현실성을 높입니다.\n",
        "\n",
        "def generate_wafer_data(num_wafers=500, timesteps=100):\n",
        "    \"\"\"\n",
        "    하나의 공정 스텝에 대한 웨이퍼 데이터를 생성합니다.\n",
        "    - FDC: 4개의 센서(압력, RF파워, 가스, 온도)에 대한 시계열 데이터\n",
        "    - MES: 각 웨이퍼의 공정 조건 및 최종 수율 정보\n",
        "    \"\"\"\n",
        "    print(f\"Generating sample data for {num_wafers} wafers...\")\n",
        "\n",
        "    # --- MES 데이터 생성 (정적 특징) ---\n",
        "    wafers = []\n",
        "    for i in range(num_wafers):\n",
        "        wafer_id = f'WAF_{i+1:04d}'\n",
        "        equipment_id = np.random.choice(['Etcher_A', 'Etcher_B', 'Etcher_C'])\n",
        "        recipe_id = np.random.choice(['Recipe_1', 'Recipe_2'])\n",
        "        wafers.append([wafer_id, equipment_id, recipe_id])\n",
        "\n",
        "    mes_df = pd.DataFrame(wafers, columns=['wafer_id', 'equipment_id', 'recipe_id'])\n",
        "\n",
        "    # --- FDC 데이터 생성 (동적 특징) ---\n",
        "    fdc_data = {}\n",
        "    anomaly_types = []\n",
        "    final_yields = []\n",
        "\n",
        "    for i, row in mes_df.iterrows():\n",
        "        wafer_id = row['wafer_id']\n",
        "\n",
        "        # 기본 센서 값 설정\n",
        "        pressure_mean = 100\n",
        "        rf_power_mean = 500\n",
        "        gas_flow_mean = 200\n",
        "        temp_mean = 80\n",
        "\n",
        "        # 정상 데이터 생성\n",
        "        pressure = np.random.normal(pressure_mean, 1, timesteps)\n",
        "        rf_power = np.random.normal(rf_power_mean, 5, timesteps)\n",
        "        gas_flow = np.random.normal(gas_flow_mean, 2, timesteps)\n",
        "        temperature = np.random.normal(temp_mean, 0.5, timesteps)\n",
        "\n",
        "        anomaly_type = 'Normal'\n",
        "        final_yield = 1 # 1: 정상, 0: 불량\n",
        "\n",
        "        # 20%의 확률로 이상 데이터 생성\n",
        "        if np.random.rand() < 0.2:\n",
        "            final_yield = 0\n",
        "            anomaly_choice = np.random.choice(['Spike', 'Variance', 'Drift'])\n",
        "            anomaly_type = anomaly_choice\n",
        "\n",
        "            if anomaly_choice == 'Spike':\n",
        "                # 특정 센서에 스파이크(순간적인 급등/급락) 발생\n",
        "                spike_sensor = np.random.choice(['pressure', 'rf_power'])\n",
        "                spike_idx = np.random.randint(20, 80)\n",
        "                spike_magnitude = (np.random.rand() - 0.5) * 10\n",
        "                if spike_sensor == 'pressure':\n",
        "                    pressure[spike_idx] += pressure_mean * 0.1 * (1 if spike_magnitude > 0 else -1)\n",
        "                else:\n",
        "                    rf_power[spike_idx] += rf_power_mean * 0.05 * (1 if spike_magnitude > 0 else -1)\n",
        "\n",
        "            elif anomaly_choice == 'Variance':\n",
        "                # 특정 센서의 변동성(분산) 증가\n",
        "                var_sensor = np.random.choice(['gas_flow', 'temperature'])\n",
        "                if var_sensor == 'gas_flow':\n",
        "                    gas_flow = np.random.normal(gas_flow_mean, 6, timesteps) # 표준편차 3배 증가\n",
        "                else:\n",
        "                    temperature = np.random.normal(temp_mean, 1.5, timesteps) # 표준편차 3배 증가\n",
        "\n",
        "            elif anomaly_choice == 'Drift':\n",
        "                # 특정 센서 값이 서서히 변하는 드리프트 현상\n",
        "                drift_sensor = np.random.choice(['pressure', 'rf_power'])\n",
        "                drift_amount = np.linspace(0, (np.random.rand() - 0.5) * 10, timesteps)\n",
        "                if drift_sensor == 'pressure':\n",
        "                    pressure += drift_amount\n",
        "                else:\n",
        "                    rf_power += drift_amount * 5\n",
        "\n",
        "        fdc_data[wafer_id] = {\n",
        "            'pressure': pressure,\n",
        "            'rf_power': rf_power,\n",
        "            'gas_flow': gas_flow,\n",
        "            'temperature': temperature\n",
        "        }\n",
        "        anomaly_types.append(anomaly_type)\n",
        "        final_yields.append(final_yield)\n",
        "\n",
        "    mes_df['anomaly_type'] = anomaly_types\n",
        "    mes_df['final_yield'] = final_yields\n",
        "\n",
        "    print(\"Sample data generation complete.\")\n",
        "    return mes_df, fdc_data\n",
        "\n",
        "# 데이터 생성 실행\n",
        "mes_df, fdc_data = generate_wafer_data(num_wafers=500, timesteps=100)\n",
        "\n",
        "# 생성된 데이터 확인\n",
        "print(\"\\n[Generated MES Data Sample (Top 5)]\")\n",
        "print(mes_df.head())\n",
        "print(f\"\\n[Normal/Anomaly Data Distribution]\\n{mes_df['anomaly_type'].value_counts()}\")\n",
        "\n",
        "# 샘플 데이터 시각화\n",
        "print(\"\\n[Sample Data Visualization] - FDC Data for Normal and Anomaly Types\")\n",
        "fig, axes = plt.subplots(4, 4, figsize=(20, 16), constrained_layout=True)\n",
        "fig.suptitle('FDC Time Series Data Samples (Normal vs. Anomaly Types)', fontsize=20)\n",
        "anomaly_types_to_plot = ['Normal', 'Spike', 'Variance', 'Drift']\n",
        "\n",
        "for i, anomaly in enumerate(anomaly_types_to_plot):\n",
        "    sample_wafer_id = mes_df[mes_df['anomaly_type'] == anomaly]['wafer_id'].iloc[0]\n",
        "    axes[0, i].plot(fdc_data[sample_wafer_id]['pressure'])\n",
        "    axes[0, i].set_title(f'Type: {anomaly}\\n(Pressure)', fontsize=12)\n",
        "    axes[1, i].plot(fdc_data[sample_wafer_id]['rf_power'])\n",
        "    axes[1, i].set_title('RF Power', fontsize=12)\n",
        "    axes[2, i].plot(fdc_data[sample_wafer_id]['gas_flow'])\n",
        "    axes[2, i].set_title('Gas Flow', fontsize=12)\n",
        "    axes[3, i].plot(fdc_data[sample_wafer_id]['temperature'])\n",
        "    axes[3, i].set_title('Temperature', fontsize=12)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2단계: 시계열 특징 공학 및 데이터 통합\n",
        "# ==============================================================================\n",
        "# FDC 시계열 데이터에서 통계적 특징(동적 특징)을 추출하고,\n",
        "# 이를 MES 데이터(정적 특징)와 결합하여 모델링을 위한 최종 데이터셋을 구축합니다.\n",
        "\n",
        "def feature_engineering(mes_df, fdc_data):\n",
        "    \"\"\" 시계열 데이터로부터 통계적 특징을 추출합니다. \"\"\"\n",
        "    print(\"\\nStarting time-series feature engineering...\")\n",
        "\n",
        "    features_list = []\n",
        "    for wafer_id in mes_df['wafer_id']:\n",
        "        wafer_features = {'wafer_id': wafer_id}\n",
        "        for sensor, series in fdc_data[wafer_id].items():\n",
        "            wafer_features[f'{sensor}_mean'] = np.mean(series)\n",
        "            wafer_features[f'{sensor}_std'] = np.std(series)\n",
        "            wafer_features[f'{sensor}_max'] = np.max(series)\n",
        "            wafer_features[f'{sensor}_min'] = np.min(series)\n",
        "            wafer_features[f'{sensor}_skew'] = pd.Series(series).skew()\n",
        "            wafer_features[f'{sensor}_kurt'] = pd.Series(series).kurtosis()\n",
        "        features_list.append(wafer_features)\n",
        "\n",
        "    features_df = pd.DataFrame(features_list)\n",
        "    print(\"Feature extraction complete.\")\n",
        "\n",
        "    # 동적 특징과 정적 특징 결합\n",
        "    final_df = pd.merge(mes_df, features_df, on='wafer_id')\n",
        "    print(\"Dynamic and static data integration complete.\")\n",
        "    return final_df\n",
        "\n",
        "# 특징 공학 실행\n",
        "final_df = feature_engineering(mes_df, fdc_data)\n",
        "\n",
        "print(\"\\n[Final Integrated Dataset Sample (Top 5)]\")\n",
        "print(final_df.head())\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3단계: 비선형 이상 탐지 (오토인코더 모델)\n",
        "# ==============================================================================\n",
        "# 정상 데이터만을 사용하여 오토인코더 모델을 학습하고,\n",
        "# 이를 통해 새로운 데이터의 이상 점수(복원 오류)를 계산합니다.\n",
        "\n",
        "# --- 데이터 전처리 ---\n",
        "# 모델에 사용할 특징들을 선택합니다. wafer_id, anomaly_type 등은 제외합니다.\n",
        "features_for_model = [col for col in final_df.columns if col not in ['wafer_id', 'anomaly_type', 'final_yield']]\n",
        "categorical_features = ['equipment_id', 'recipe_id']\n",
        "numerical_features = [col for col in features_for_model if col not in categorical_features]\n",
        "\n",
        "# 범주형 데이터는 원-핫 인코딩, 수치형 데이터는 Min-Max 스케일링을 적용합니다.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough' # 처리하지 않는 컬럼은 그대로 둠\n",
        ")\n",
        "\n",
        "# 정상 데이터(final_yield=1)와 이상 데이터(final_yield=0) 분리\n",
        "normal_df = final_df[final_df['final_yield'] == 1]\n",
        "anomaly_df = final_df[final_df['final_yield'] == 0]\n",
        "\n",
        "# 정상 데이터를 학습용과 검증용으로 분리\n",
        "X_train, X_val = train_test_split(normal_df[features_for_model], test_size=0.2, random_state=42)\n",
        "\n",
        "# 전체 데이터를 대상으로 전처리기(preprocessor)를 학습(fit)시킵니다.\n",
        "# 이는 모든 데이터에 일관된 변환을 적용하기 위함입니다.\n",
        "X = final_df[features_for_model]\n",
        "preprocessor.fit(X)\n",
        "\n",
        "# 데이터 변환\n",
        "X_train_scaled = preprocessor.transform(X_train)\n",
        "X_val_scaled = preprocessor.transform(X_val)\n",
        "X_all_scaled = preprocessor.transform(X)\n",
        "\n",
        "print(f\"\\nTraining data shape: {X_train_scaled.shape}\")\n",
        "\n",
        "# --- 오토인코더 모델 정의 ---\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "encoding_dim = int(input_dim / 2)\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "# 인코더\n",
        "encoder = Dense(encoding_dim, activation=\"tanh\")(input_layer)\n",
        "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
        "# 잠재 공간\n",
        "latent_view = Dense(int(encoding_dim / 4), activation=\"relu\")(encoder)\n",
        "# 디코더\n",
        "decoder = Dense(int(encoding_dim / 2), activation='tanh')(latent_view)\n",
        "decoder = Dense(encoding_dim, activation='tanh')(decoder)\n",
        "# 출력층\n",
        "output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "autoencoder.compile(optimizer='adam', loss='mae') # Mean Absolute Error 사용\n",
        "autoencoder.summary()\n",
        "\n",
        "# --- 모델 학습 ---\n",
        "print(\"\\nStarting Autoencoder model training...\")\n",
        "history = autoencoder.fit(\n",
        "    X_train_scaled, X_train_scaled,\n",
        "    epochs=50,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    validation_data=(X_val_scaled, X_val_scaled),\n",
        "    verbose=1 # 학습 과정 출력\n",
        ").history\n",
        "\n",
        "# 학습 손실 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Autoencoder Model Loss', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- 이상 점수 계산 및 임계치 설정 ---\n",
        "# 학습된 모델로 전체 데이터를 재구성(복원)\n",
        "X_pred_scaled = autoencoder.predict(X_all_scaled)\n",
        "# 원본과 복원본의 차이(MAE)를 이상 점수로 계산\n",
        "mae = np.mean(np.abs(X_all_scaled - X_pred_scaled), axis=1)\n",
        "final_df['reconstruction_error'] = mae\n",
        "\n",
        "# 학습에 사용된 정상 데이터의 복원 오류를 기반으로 임계치 설정\n",
        "# (예: 정상 데이터 복원 오류의 99분위수를 임계치로 사용)\n",
        "normal_train_pred = autoencoder.predict(X_train_scaled)\n",
        "train_mae = np.mean(np.abs(X_train_scaled - normal_train_pred), axis=1)\n",
        "threshold = np.quantile(train_mae, 0.99)\n",
        "\n",
        "print(f\"\\nCalculated Anomaly Detection Threshold: {threshold:.4f}\")\n",
        "\n",
        "# 복원 오류 분포 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(final_df[final_df['final_yield'] == 1]['reconstruction_error'], color=\"blue\", label=\"Normal\", kde=True, stat=\"density\", linewidth=0)\n",
        "sns.histplot(final_df[final_df['final_yield'] == 0]['reconstruction_error'], color=\"red\", label=\"Anomaly\", kde=True, stat=\"density\", linewidth=0)\n",
        "plt.axvline(threshold, color='green', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
        "plt.title('Reconstruction Error Distribution for Normal vs. Anomaly Data', fontsize=16)\n",
        "plt.xlabel('Reconstruction Error', fontsize=12)\n",
        "plt.ylabel('Density', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- 모델 평가 ---\n",
        "y_true = final_df['final_yield'] == 0  # 0(불량)이면 True(이상)\n",
        "y_pred = final_df['reconstruction_error'] > threshold\n",
        "\n",
        "print(\"\\n[Anomaly Detection Model Evaluation Results]\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly']))\n",
        "\n",
        "# 혼동 행렬(Confusion Matrix) 시각화\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'Anomaly'],\n",
        "            yticklabels=['Normal', 'Anomaly'])\n",
        "plt.title('Confusion Matrix', fontsize=16)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4단계: 원인 변수 설명 (Explainable AI with SHAP)\n",
        "# ==============================================================================\n",
        "# 이상으로 탐지된 데이터에 대해, 어떤 특징이 이상 판단에 큰 영향을 미쳤는지\n",
        "# SHAP을 이용하여 분석하고 시각화합니다.\n",
        "\n",
        "# SHAP는 원본 스케일의 데이터를 사용하는 것이 해석에 용이합니다.\n",
        "# 전처리된 데이터를 다시 원래 스케일로 되돌리는 것은 복잡하므로,\n",
        "# 여기서는 전처리된 데이터를 기반으로 설명하되, 특징 이름은 원래 이름을 사용합니다.\n",
        "# SHAP Explainer를 생성합니다.\n",
        "# (주의: 많은 데이터에 대해 실행 시 시간이 오래 걸릴 수 있습니다)\n",
        "\n",
        "# 원-핫 인코딩으로 생성된 특징 이름까지 포함한 전체 특징 이름 리스트 생성\n",
        "ohe_feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
        "all_feature_names = numerical_features + ohe_feature_names\n",
        "\n",
        "# SHAP Explainer 생성\n",
        "# 복원 오류(loss) 자체를 설명하기 위해 커스텀 함수를 정의합니다.\n",
        "def model_predict_loss(data):\n",
        "    pred = autoencoder.predict(data)\n",
        "    loss = np.mean(np.abs(data - pred), axis=1)\n",
        "    return loss\n",
        "\n",
        "# KernelExplainer는 모델의 종류에 상관없이 사용할 수 있는 Explainer입니다.\n",
        "# 학습 데이터 중 일부를 배경 데이터로 사용하여 설명의 기준을 잡습니다.\n",
        "background_data = shap.sample(X_train_scaled, 100) # 100개 샘플 사용\n",
        "explainer = shap.KernelExplainer(model_predict_loss, background_data)\n",
        "\n",
        "# 이상으로 탐지된 데이터 중 일부 샘플에 대해 SHAP 값 계산\n",
        "anomaly_samples_scaled = preprocessor.transform(final_df[y_pred][features_for_model].head(5))\n",
        "shap_values = explainer.shap_values(anomaly_samples_scaled)\n",
        "\n",
        "print(\"\\nStarting SHAP analysis (for top 5 anomaly samples)...\")\n",
        "# 분석 결과 시각화\n",
        "X_anomaly_df = pd.DataFrame(anomaly_samples_scaled, columns=all_feature_names)\n",
        "\n",
        "# 각 이상 샘플에 대한 Force Plot\n",
        "print(\"SHAP Force Plot for each anomaly sample:\")\n",
        "print(\"(The plot shows how each feature contributed to pushing the model output from the base value (the average prediction) to the model output.)\")\n",
        "print(\"(Features pushing the prediction higher are in red, those pushing it lower are in blue.)\")\n",
        "\n",
        "for i in range(len(shap_values)):\n",
        "    shap.force_plot(\n",
        "        explainer.expected_value,\n",
        "        shap_values[i, :],\n",
        "        X_anomaly_df.iloc[i, :],\n",
        "        matplotlib=True,\n",
        "        show=True\n",
        "    )\n",
        "\n",
        "# 전체 이상 샘플에 대한 요약(Summary) Plot\n",
        "print(\"\\nSHAP Summary Plot for all anomaly samples:\")\n",
        "print(\"(This plot reveals which features are most important for anomaly detection overall.)\")\n",
        "print(\"(The color represents the feature value (red is high, blue is low), and the x-axis is the SHAP value.)\")\n",
        "shap.summary_plot(shap_values, X_anomaly_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vr1OBaeZXOoI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}